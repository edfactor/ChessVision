{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain pre-trained MNIST classifier\n",
    "\n",
    "Using the model from https://github.com/EN10/KerasMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/chessvision/lib/python3.5/site-packages/keras/models.py:291: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "from cv_globals import CVROOT\n",
    "\n",
    "model = load_model(CVROOT + \"/weights/cnn.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x1c250659b0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(13, activation=\"softmax\", name=\"final_dense\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional.Conv2D at 0x1c497aa518>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new_model Sequential()\n",
    "\n",
    "#= Conv2D(16, (3, 3), input_shape=(64, 64, 1))\n",
    "\n",
    "model.layers[0]\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data stuff\n",
    "\n",
    "import quilt\n",
    "import cv2\n",
    "from sklearn.utils import class_weight\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "labels = {\"b\": 6, \"k\": 7, \"n\": 8, \"p\": 9, \"q\": 10, \"r\": 11, \"B\": 0,\n",
    "          \"f\": 12, \"K\": 1, \"N\": 2, \"P\": 3, \"Q\": 4, \"R\": 5}\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=5,\n",
    "    zoom_range=0.05,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=5\n",
    ")\n",
    "valid_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    ")\n",
    "\n",
    "def install_data():\n",
    "    quilt.install(\"gudbrandtandberg/chesspieces\", force=True)\n",
    "\n",
    "def get_data(node, N):\n",
    "    X = np.zeros((N, 64, 64, 1))\n",
    "    y = np.zeros((N))\n",
    "    i = 0\n",
    "    dirs = node._group_keys()\n",
    "    for dir_name, dir_node in zip(dirs, node):\n",
    "        label = labels[dir_name]\n",
    "        for img in dir_node:\n",
    "            # load greyscale image\n",
    "            orig = cv2.imread(img(), 0)\n",
    "            X[i, :, :, 0] = orig\n",
    "            y[i] = label\n",
    "            i += 1\n",
    "    return X, y\n",
    "\n",
    "def keras_generator(*, transform=False):\n",
    "    def _keras_generator(node, paths):\n",
    "\n",
    "        datagen = train_datagen if transform else valid_datagen\n",
    "\n",
    "        X, y = get_data(node, len(paths))\n",
    "\n",
    "        datagen.fit(X)\n",
    "        y = to_categorical(y)\n",
    "        return datagen.flow(X, y)\n",
    "    return _keras_generator\n",
    "\n",
    "def get_training_generator():\n",
    "    from quilt.data.gudbrandtandberg import chesspieces as pieces\n",
    "    return pieces[\"training\"](asa=keras_generator(transform=True))\n",
    "\n",
    "\n",
    "def get_validation_generator():\n",
    "    from quilt.data.gudbrandtandberg import chesspieces as pieces\n",
    "    return pieces[\"validation\"](asa=keras_generator(transform=False))\n",
    "\n",
    "def labels_only(node, paths):\n",
    "    _, y = get_data(node, len(paths))\n",
    "    return y\n",
    "\n",
    "def get_class_weights():\n",
    "    from quilt.data.gudbrandtandberg import chesspieces as pieces\n",
    "    print(\"Computing class weights\")\n",
    "    y = pieces[\"training\"](asa=labels_only)\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(y), y)\n",
    "    return class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing class weights\n"
     ]
    }
   ],
   "source": [
    "#install_data()\n",
    "class_weights = get_class_weights()\n",
    "train_generator = get_training_generator()\n",
    "valid_generator = get_validation_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 2.4268 - acc: 0.2492 - val_loss: 2.2840 - val_acc: 0.3070\n",
      "Epoch 2/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 2.2957 - acc: 0.2660 - val_loss: 2.1688 - val_acc: 0.3357\n",
      "Epoch 3/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 2.2262 - acc: 0.2885 - val_loss: 2.0810 - val_acc: 0.3583\n",
      "Epoch 4/100\n",
      "255/255 [==============================] - 9s 36ms/step - loss: 2.1661 - acc: 0.3108 - val_loss: 2.0063 - val_acc: 0.3850\n",
      "Epoch 5/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 2.1158 - acc: 0.3300 - val_loss: 1.9463 - val_acc: 0.4056\n",
      "Epoch 6/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 2.0805 - acc: 0.3374 - val_loss: 1.8973 - val_acc: 0.4202\n",
      "Epoch 7/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 2.0470 - acc: 0.3576 - val_loss: 1.8544 - val_acc: 0.4298\n",
      "Epoch 8/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 2.0182 - acc: 0.3609 - val_loss: 1.8197 - val_acc: 0.4389\n",
      "Epoch 9/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.9902 - acc: 0.3702 - val_loss: 1.7895 - val_acc: 0.4479\n",
      "Epoch 10/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.9819 - acc: 0.3694 - val_loss: 1.7650 - val_acc: 0.4545\n",
      "Epoch 11/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.9642 - acc: 0.3792 - val_loss: 1.7396 - val_acc: 0.4585\n",
      "Epoch 12/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.9424 - acc: 0.3789 - val_loss: 1.7192 - val_acc: 0.4640\n",
      "Epoch 13/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.9273 - acc: 0.3828 - val_loss: 1.7010 - val_acc: 0.4675\n",
      "Epoch 14/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.9178 - acc: 0.3865 - val_loss: 1.6837 - val_acc: 0.4746\n",
      "Epoch 15/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.9078 - acc: 0.3853 - val_loss: 1.6694 - val_acc: 0.4781\n",
      "Epoch 16/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.8981 - acc: 0.3860 - val_loss: 1.6553 - val_acc: 0.4806\n",
      "Epoch 17/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.8871 - acc: 0.3897 - val_loss: 1.6439 - val_acc: 0.4811\n",
      "Epoch 18/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.8817 - acc: 0.3889 - val_loss: 1.6319 - val_acc: 0.4806\n",
      "Epoch 19/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.8587 - acc: 0.3923 - val_loss: 1.6186 - val_acc: 0.4841\n",
      "Epoch 20/100\n",
      "255/255 [==============================] - 8s 33ms/step - loss: 1.8590 - acc: 0.3961 - val_loss: 1.6089 - val_acc: 0.4862\n",
      "Epoch 21/100\n",
      "255/255 [==============================] - 8s 33ms/step - loss: 1.8557 - acc: 0.3982 - val_loss: 1.6008 - val_acc: 0.4877\n",
      "Epoch 22/100\n",
      "255/255 [==============================] - 8s 33ms/step - loss: 1.8493 - acc: 0.3965 - val_loss: 1.5938 - val_acc: 0.4877\n",
      "Epoch 23/100\n",
      "255/255 [==============================] - 8s 33ms/step - loss: 1.8393 - acc: 0.3969 - val_loss: 1.5839 - val_acc: 0.4887\n",
      "Epoch 24/100\n",
      "255/255 [==============================] - 8s 33ms/step - loss: 1.8356 - acc: 0.3958 - val_loss: 1.5773 - val_acc: 0.4922\n",
      "Epoch 25/100\n",
      "255/255 [==============================] - 10s 38ms/step - loss: 1.8341 - acc: 0.3984 - val_loss: 1.5702 - val_acc: 0.4927\n",
      "Epoch 26/100\n",
      "255/255 [==============================] - 9s 36ms/step - loss: 1.8263 - acc: 0.3992 - val_loss: 1.5639 - val_acc: 0.4922\n",
      "Epoch 27/100\n",
      "255/255 [==============================] - 10s 37ms/step - loss: 1.8209 - acc: 0.4018 - val_loss: 1.5594 - val_acc: 0.4927\n",
      "Epoch 28/100\n",
      "255/255 [==============================] - 10s 38ms/step - loss: 1.8247 - acc: 0.4044 - val_loss: 1.5536 - val_acc: 0.4942\n",
      "Epoch 29/100\n",
      "255/255 [==============================] - 10s 41ms/step - loss: 1.8137 - acc: 0.4042 - val_loss: 1.5447 - val_acc: 0.4937\n",
      "Epoch 30/100\n",
      "255/255 [==============================] - 12s 47ms/step - loss: 1.8147 - acc: 0.4053 - val_loss: 1.5390 - val_acc: 0.4992\n",
      "Epoch 31/100\n",
      "255/255 [==============================] - 9s 37ms/step - loss: 1.8029 - acc: 0.4098 - val_loss: 1.5331 - val_acc: 0.4982\n",
      "Epoch 32/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.8092 - acc: 0.4043 - val_loss: 1.5305 - val_acc: 0.4982\n",
      "Epoch 33/100\n",
      "255/255 [==============================] - 9s 36ms/step - loss: 1.7980 - acc: 0.4116 - val_loss: 1.5270 - val_acc: 0.5008\n",
      "Epoch 34/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.8064 - acc: 0.4089 - val_loss: 1.5224 - val_acc: 0.5038\n",
      "Epoch 35/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7906 - acc: 0.4153 - val_loss: 1.5176 - val_acc: 0.5043\n",
      "Epoch 36/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7837 - acc: 0.4084 - val_loss: 1.5127 - val_acc: 0.5068\n",
      "Epoch 37/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7816 - acc: 0.4154 - val_loss: 1.5088 - val_acc: 0.5058\n",
      "Epoch 38/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7875 - acc: 0.4128 - val_loss: 1.5066 - val_acc: 0.5048\n",
      "Epoch 39/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7796 - acc: 0.4088 - val_loss: 1.5011 - val_acc: 0.5028\n",
      "Epoch 40/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7723 - acc: 0.4156 - val_loss: 1.4984 - val_acc: 0.5058\n",
      "Epoch 41/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7750 - acc: 0.4191 - val_loss: 1.4948 - val_acc: 0.5113\n",
      "Epoch 42/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7743 - acc: 0.4156 - val_loss: 1.4907 - val_acc: 0.5113\n",
      "Epoch 43/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7713 - acc: 0.4212 - val_loss: 1.4875 - val_acc: 0.5133\n",
      "Epoch 44/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7709 - acc: 0.4183 - val_loss: 1.4848 - val_acc: 0.5169\n",
      "Epoch 45/100\n",
      "255/255 [==============================] - 9s 36ms/step - loss: 1.7745 - acc: 0.4188 - val_loss: 1.4815 - val_acc: 0.5209\n",
      "Epoch 46/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7727 - acc: 0.4208 - val_loss: 1.4791 - val_acc: 0.5194\n",
      "Epoch 47/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7619 - acc: 0.4203 - val_loss: 1.4774 - val_acc: 0.5169\n",
      "Epoch 48/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7627 - acc: 0.4218 - val_loss: 1.4744 - val_acc: 0.5214\n",
      "Epoch 49/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7611 - acc: 0.4230 - val_loss: 1.4728 - val_acc: 0.5254\n",
      "Epoch 50/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7646 - acc: 0.4164 - val_loss: 1.4689 - val_acc: 0.5264\n",
      "Epoch 51/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7644 - acc: 0.4214 - val_loss: 1.4655 - val_acc: 0.5289\n",
      "Epoch 52/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7533 - acc: 0.4238 - val_loss: 1.4630 - val_acc: 0.5310\n",
      "Epoch 53/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7553 - acc: 0.4290 - val_loss: 1.4596 - val_acc: 0.5350\n",
      "Epoch 54/100\n",
      "255/255 [==============================] - 9s 36ms/step - loss: 1.7608 - acc: 0.4245 - val_loss: 1.4579 - val_acc: 0.5360\n",
      "Epoch 55/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7505 - acc: 0.4228 - val_loss: 1.4568 - val_acc: 0.5365\n",
      "Epoch 56/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7523 - acc: 0.4277 - val_loss: 1.4525 - val_acc: 0.5375\n",
      "Epoch 57/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7531 - acc: 0.4324 - val_loss: 1.4512 - val_acc: 0.5380\n",
      "Epoch 58/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7540 - acc: 0.4299 - val_loss: 1.4490 - val_acc: 0.5400\n",
      "Epoch 59/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7430 - acc: 0.4301 - val_loss: 1.4468 - val_acc: 0.5420\n",
      "Epoch 60/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7481 - acc: 0.4259 - val_loss: 1.4445 - val_acc: 0.5410\n",
      "Epoch 61/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7445 - acc: 0.4277 - val_loss: 1.4412 - val_acc: 0.5445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7400 - acc: 0.4321 - val_loss: 1.4418 - val_acc: 0.5410\n",
      "Epoch 63/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7329 - acc: 0.4306 - val_loss: 1.4384 - val_acc: 0.5435\n",
      "Epoch 64/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7516 - acc: 0.4272 - val_loss: 1.4384 - val_acc: 0.5471\n",
      "Epoch 65/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7414 - acc: 0.4283 - val_loss: 1.4366 - val_acc: 0.5481\n",
      "Epoch 66/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.7327 - acc: 0.4341 - val_loss: 1.4350 - val_acc: 0.5491\n",
      "Epoch 67/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.7327 - acc: 0.4355 - val_loss: 1.4337 - val_acc: 0.5460\n",
      "Epoch 68/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.7406 - acc: 0.4306 - val_loss: 1.4318 - val_acc: 0.5440\n",
      "Epoch 69/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.7464 - acc: 0.4314 - val_loss: 1.4294 - val_acc: 0.5460\n",
      "Epoch 70/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.7376 - acc: 0.4343 - val_loss: 1.4271 - val_acc: 0.5460\n",
      "Epoch 71/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7288 - acc: 0.4356 - val_loss: 1.4258 - val_acc: 0.5486\n",
      "Epoch 72/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.7344 - acc: 0.4335 - val_loss: 1.4247 - val_acc: 0.5521\n",
      "Epoch 73/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.7365 - acc: 0.4348 - val_loss: 1.4220 - val_acc: 0.5526\n",
      "Epoch 74/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.7362 - acc: 0.4323 - val_loss: 1.4193 - val_acc: 0.5531\n",
      "Epoch 75/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.7211 - acc: 0.4435 - val_loss: 1.4173 - val_acc: 0.5561\n",
      "Epoch 76/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.7367 - acc: 0.4330 - val_loss: 1.4155 - val_acc: 0.5536\n",
      "Epoch 77/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.7302 - acc: 0.4387 - val_loss: 1.4148 - val_acc: 0.5601\n",
      "Epoch 78/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.7438 - acc: 0.4318 - val_loss: 1.4114 - val_acc: 0.5581\n",
      "Epoch 79/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.7234 - acc: 0.4428 - val_loss: 1.4090 - val_acc: 0.5606\n",
      "Epoch 80/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.7141 - acc: 0.4408 - val_loss: 1.4044 - val_acc: 0.5601\n",
      "Epoch 81/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7146 - acc: 0.4422 - val_loss: 1.4019 - val_acc: 0.5591\n",
      "Epoch 82/100\n",
      "255/255 [==============================] - 8s 33ms/step - loss: 1.7276 - acc: 0.4384 - val_loss: 1.3998 - val_acc: 0.5606\n",
      "Epoch 83/100\n",
      "255/255 [==============================] - 8s 33ms/step - loss: 1.7148 - acc: 0.4474 - val_loss: 1.3962 - val_acc: 0.5637\n",
      "Epoch 84/100\n",
      "255/255 [==============================] - 8s 33ms/step - loss: 1.7174 - acc: 0.4427 - val_loss: 1.3925 - val_acc: 0.5672\n",
      "Epoch 85/100\n",
      "255/255 [==============================] - 10s 40ms/step - loss: 1.7147 - acc: 0.4444 - val_loss: 1.3888 - val_acc: 0.5717\n",
      "Epoch 86/100\n",
      "255/255 [==============================] - 10s 38ms/step - loss: 1.7101 - acc: 0.4494 - val_loss: 1.3863 - val_acc: 0.5692\n",
      "Epoch 87/100\n",
      "255/255 [==============================] - 10s 40ms/step - loss: 1.7145 - acc: 0.4420 - val_loss: 1.3824 - val_acc: 0.5712\n",
      "Epoch 88/100\n",
      "255/255 [==============================] - 10s 38ms/step - loss: 1.7090 - acc: 0.4412 - val_loss: 1.3794 - val_acc: 0.5722\n",
      "Epoch 89/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7080 - acc: 0.4432 - val_loss: 1.3760 - val_acc: 0.5773\n",
      "Epoch 90/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7252 - acc: 0.4427 - val_loss: 1.3743 - val_acc: 0.5752\n",
      "Epoch 91/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7033 - acc: 0.4457 - val_loss: 1.3714 - val_acc: 0.5752\n",
      "Epoch 92/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7041 - acc: 0.4513 - val_loss: 1.3703 - val_acc: 0.5778\n",
      "Epoch 93/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7083 - acc: 0.4544 - val_loss: 1.3667 - val_acc: 0.5778\n",
      "Epoch 94/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7053 - acc: 0.4444 - val_loss: 1.3651 - val_acc: 0.5808\n",
      "Epoch 95/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.7043 - acc: 0.4484 - val_loss: 1.3619 - val_acc: 0.5808\n",
      "Epoch 96/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.6877 - acc: 0.4600 - val_loss: 1.3589 - val_acc: 0.5793\n",
      "Epoch 97/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.6952 - acc: 0.4545 - val_loss: 1.3557 - val_acc: 0.5828\n",
      "Epoch 98/100\n",
      "255/255 [==============================] - 9s 34ms/step - loss: 1.6921 - acc: 0.4538 - val_loss: 1.3516 - val_acc: 0.5823\n",
      "Epoch 99/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.6946 - acc: 0.4506 - val_loss: 1.3501 - val_acc: 0.5843\n",
      "Epoch 100/100\n",
      "255/255 [==============================] - 9s 35ms/step - loss: 1.6885 - acc: 0.4552 - val_loss: 1.3506 - val_acc: 0.5828\n",
      "Training the square classifier took 14 minutes and 58 seconds\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import time\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                           patience=8,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4),\n",
    "             ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=4,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4),\n",
    "             ModelCheckpoint(monitor='val_loss',\n",
    "                             filepath=CVROOT + \"/weights/transfer_model.h5\",\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=False)]\n",
    "                       \n",
    "start = time.time()\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=len(train_generator),\n",
    "                    epochs=100,\n",
    "                    class_weight=class_weights,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=len(valid_generator))\n",
    "                       \n",
    "duration = time.time() - start\n",
    "print(\"Training the square classifier took {} minutes and {} seconds\".format(\n",
    "    int(np.floor(duration / 60)), int(np.round(duration % 60))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
